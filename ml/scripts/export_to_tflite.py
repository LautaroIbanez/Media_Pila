#!/usr/bin/env python3
"""
Script para exportar modelos entrenados a TensorFlow Lite.

Soporta:
    - Cuantizaci√≥n post-entrenamiento
    - Cuantizaci√≥n completa int8
    - Optimizaci√≥n de GPU

Uso:
    python export_to_tflite.py --model_path models/sock_detector.pb --output_path models/sock_detector.tflite
"""

import argparse
from pathlib import Path
import tensorflow as tf
import numpy as np

def parse_args():
    """Parse argumentos de l√≠nea de comandos."""
    parser = argparse.ArgumentParser(description='Exportar modelo a TFLite')
    parser.add_argument('--model_path', type=str, required=True,
                        help='Ruta al modelo SavedModel o .pb')
    parser.add_argument('--output_path', type=str, required=True,
                        help='Ruta de salida para .tflite')
    parser.add_argument('--quantize', action='store_true',
                        help='Aplicar cuantizaci√≥n din√°mica')
    parser.add_argument('--quantize_full', action='store_true',
                        help='Aplicar cuantizaci√≥n completa int8')
    parser.add_argument('--optimize_gpu', action='store_true',
                        help='Optimizar para GPU')
    return parser.parse_args()

def create_representative_dataset(input_shape, num_samples=100):
    """Crea dataset representativo para cuantizaci√≥n."""
    def representative_data_gen():
        for _ in range(num_samples):
            # Generar datos representativos (im√°genes de medias)
            data = np.random.rand(1, *input_shape).astype(np.float32)
            yield [data]
    return representative_data_gen

def export_to_tflite(args):
    """Exporta modelo a TensorFlow Lite con optimizaciones."""
    print("\nüì¶ Exportando modelo a TensorFlow Lite...")
    print(f"   Modelo: {args.model_path}")
    print(f"   Salida: {args.output_path}")
    
    # Verificar que el modelo existe
    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"   ‚ùå Error: {model_path} no existe")
        return False
    
    try:
        # Cargar modelo
        print("   üì• Cargando modelo...")
        if model_path.is_dir():
            # SavedModel
            model = tf.saved_model.load(str(model_path))
            converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))
        else:
            # Archivo .pb o .h5
            if str(model_path).endswith('.h5'):
                model = tf.keras.models.load_model(str(model_path))
                converter = tf.lite.TFLiteConverter.from_keras_model(model)
            else:
                # Intentar cargar como SavedModel
                converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))
        
        # Configurar optimizaciones
        if args.quantize:
            print("   ‚úÖ Cuantizaci√≥n din√°mica habilitada")
            print("      - Reduce tama√±o ~4x")
            print("      - Inferencia ~2-3x m√°s r√°pida")
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
        if args.quantize_full:
            print("   ‚úÖ Cuantizaci√≥n completa int8 habilitada")
            print("      - Reduce tama√±o ~4x")
            print("      - Inferencia muy r√°pida en CPU")
            print("      - Requiere dataset representativo")
            
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
            
            # Dataset representativo para cuantizaci√≥n
            # Asumir input shape t√≠pico para detecci√≥n de objetos
            input_shape = (320, 320, 3)  # Ajustar seg√∫n el modelo
            converter.representative_dataset = create_representative_dataset(input_shape)
        
        if args.optimize_gpu:
            print("   ‚úÖ Optimizaci√≥n GPU habilitada")
            print("      - GPU delegate para inferencia")
            # Nota: La optimizaci√≥n GPU se aplica en tiempo de ejecuci√≥n, no en conversi√≥n
        
        # Configuraciones adicionales
        converter.allow_custom_ops = True
        converter.experimental_new_converter = True
        
        # Convertir
        print("   üîÑ Convirtiendo modelo...")
        tflite_model = converter.convert()
        
        # Guardar
        output_path = Path(args.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        # Informaci√≥n del modelo
        model_size = len(tflite_model) / (1024 * 1024)  # MB
        print(f"   ‚úÖ Modelo exportado exitosamente")
        print(f"      - Tama√±o: {model_size:.2f} MB")
        print(f"      - Ubicaci√≥n: {output_path}")
        
        # Validar modelo
        print("   üîç Validando modelo...")
        interpreter = tf.lite.Interpreter(model_path=str(output_path))
        interpreter.allocate_tensors()
        
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        print(f"      - Inputs: {len(input_details)}")
        for i, detail in enumerate(input_details):
            print(f"        Input {i}: {detail['shape']} ({detail['dtype']})")
        
        print(f"      - Outputs: {len(output_details)}")
        for i, detail in enumerate(output_details):
            print(f"        Output {i}: {detail['shape']} ({detail['dtype']})")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Error durante la exportaci√≥n: {str(e)}")
        print("   üí° Sugerencias:")
        print("      - Verifica que el modelo est√© en formato SavedModel o .h5")
        print("      - Aseg√∫rate de que TensorFlow est√© instalado correctamente")
        print("      - Para cuantizaci√≥n completa, proporciona datos representativos")
        return False

def main():
    """Funci√≥n principal."""
    args = parse_args()
    
    print("üöÄ Exportando modelo a TensorFlow Lite...")
    
    # Verificar que el modelo exista
    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"‚ùå Error: {model_path} no existe")
        return
    
    # Crear directorio de salida
    output_path = Path(args.output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Exportar
    success = export_to_tflite(args)
    
    if success:
        print(f"\n‚úÖ Modelo exportado exitosamente")
        print(f"\nüí° Siguiente paso:")
        print(f"   1. Copia el .tflite a: app/src/main/assets/")
        print(f"   2. Actualiza build.gradle.kts con dependencias TFLite")
        print(f"   3. Crea wrapper en app/src/main/java/com/example/media_pila/ml/")

if __name__ == '__main__':
    main()



